---
title: "TranslationABTest"
output: html_document
---
```{r}
# Libraries needed
library(dplyr)
library(rpart)
library(ggplot2)
```
##Load Dataset
```{r}
#read data
user = read.csv("C:/Users/siyue/OneDrive/Desktop/Machine Learning/Data/TranslationABTest/user_table.csv")
test = read.csv("C:/Users/siyue/OneDrive/Desktop/Machine Learning/Data/TranslationABTest/test_table.csv")
```
##Create one dataset
```{r}
# Are there dupes in test?
length(unique(test$user_id)) == length(test$user_id)
```

```{r}
# Are there dupes in user?
length(unique(user$user_id)) == length(user$user_id)
```

```{r}
# Is everyone in one table also in the other one?
length(user$user_id) - length(test$user_id)
```
We have some user ids missing in user table. When joining, we have to be careful to do not lose the user ids in the test table, but not in the user table.
```{r}
data = merge(test,user,by = "user_id", all.x = TRUE) #In this way we don't lose data
data$date = as.Date(data$date)
summary(data)
```
First of all, we should make sure it is true Spain converts much better than the rest of LatAm countries.
```{r}
data_conversion_country = data %>% 
                            group_by(country) %>%
                            summarize(conversion = mean(conversion[test == 0])) %>% #Check the old version
                            arrange (desc(conversion))

head(data_conversion_country)
```
We can see that it is true that Spain converts much better.

##Analysis
```{r}
#We can use a simple t-test here.
#Nothing changed in Spain, so we can remove those users.
data_test = subset(data, country != "Spain")
t.test(data_test$conversion[data_test$test == 1], data_test$conversion[data_test$test == 0])
```
The result shows that not in the test are converting at 4.8% while users in the test just at 4.3%. That's a 10% drop, which is unreasonable. There might be two reasons for weired A/B test results: 1. We didn't collect enough data. 2. Some omitted variable bias has been introduced in the experiment so that test/control people are not really randomly selected.

Firstly, let's plot day by day, to see if these weired results have been constantly happening or they just started happening all of a sudden.
```{r}
data_test_by_day = data_test %>%
                    group_by(date) %>%
                    summarize(test_vs_control = mean(conversion[test == 1])/
                                                mean(conversion[test == 0])
                              )
qplot(date, test_vs_control, data = data_test_by_day, geom = 'line')
```
From the plot, we notice a couple of things:

1. Test has constantly been worse than control and there is relatively little variance across days. That probably means that we do have enough data, but there was some bias in the experiment set up.
2. On a side note, we just ran it for 5 days. We should always run the test for at least 1 full week to capture weekly patterns, 2 weeks would be much better. 

Time to find out the bias! Likely, there is for some reason some segment of users􀀁more likely to end􀀁up in test or in control, this segment had a significantly above/below conversion rate and this􀀁affected the overall results.

In an ideal world, the distribution of people in test and control for each segment should be the same.There are many ways to check whis. One way is to build a decision tree where the outcome variable is whether the user is in test or control. If the tree splits, it means􀀁 that for given values of that variable you are more likely to end up in test or control. But this should be􀀁 impossible! Therefore, if the randomization worked, the tree should not split at all (or at least not be able to􀀁
separate the two classes well).
Let’s check this:
```{r}
tree = rpart(test ~., data_test[,-8], control = rpart.control(minbucket = nrow(data_test)/100, maxdepth = 2))
#We only look for segments representing at least 1% of the populations.
tree
```
###For some countries, randomization fails, So we should control country variable.
The randomization is perfect for the countries on one side of the split (country=Bolivia, Chile, Colombia, Costa Rica, Ecuador, EL Salvador, Guatemala, Honduras, Mexico, Nicaragua, Panama, Paraguay, Peru, Venezuela). Indeed, in that leaf the test/control ratio is 0.498! However, Argentina and Uruguay together have 80% test and 20% control! So let’s check the test results after controlling for country. That is, we check for each country how the test is doing to avoid omitted variable bias:
```{r}
data_test_country = data_test %>%
                    group_by(country) %>%
                    summarize(p_value = t.test(conversion[test==1], conversion[test==0])$p.value,
                              conversion_test = t.test(conversion[test==1], conversion[test==0])$estimate[1],
                              conversion_control = t.test(conversion[test==1], conversion[test==0])$estimate[2]
                              ) %>%
                    arrange (p_value)

data_test_country
```
##Conclusion
After we control for country, the test clearly appears non significant. Not a great success given that the goal was to improve conversion rate, but we know that a localized translation didn’t make things worse.
